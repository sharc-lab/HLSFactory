{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPz8sWSZOICR",
    "outputId": "c818f262-a362-4f21-c77b-35bf50471650"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from torch import Tensor\n",
    "from typing import Dict, Iterable, Callable\n",
    "import struct\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Change to True to generate corresponding values\n",
    "write_layer_outputs_to_file = False\n",
    "write_layer_params_to_file = False\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "inp = torch.ones(1, 3, 736, 1280)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"fused_resnet_bin/\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"fused_resnet_bin/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )\n",
    "            bias_filename = (\n",
    "                \"fused_resnet_bin/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yto2UUy-Of70",
    "outputId": "4623b9a8-8e04-43b1-c192-48c37e33bdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-YOLOv4'...\n",
      "remote: Enumerating objects: 1049, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 1049 (delta 2), reused 0 (delta 0), pack-reused 1043\u001b[K\n",
      "Receiving objects: 100% (1049/1049), 2.39 MiB | 16.42 MiB/s, done.\n",
      "Resolving deltas: 100% (644/644), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Tianxiaomo/pytorch-YOLOv4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20NfLHCyPVDI",
    "outputId": "906e9b99-93de-4fc9-a4a8-252ea9556d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "drive  pytorch-YOLOv4  sample_data\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "import os\n",
    "\n",
    "os.chdir(\"pytorch-YOLOv4\")\n",
    "from tool.darknet2pytorch import Darknet\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_xOMMx1Orhy"
   },
   "outputs": [],
   "source": [
    "m = Darknet(\"./cfg/yolov4-tiny.cfg\")\n",
    "m.load_weights(\"../yolov4-tiny.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRl22T3qO3NZ",
    "outputId": "70c892b3-b313-46a5-944b-cb8d0242c9b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (models): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): EmptyModule()\n",
       "    (4): Sequential(\n",
       "      (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): EmptyModule()\n",
       "    (7): Sequential(\n",
       "      (conv6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): EmptyModule()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Sequential(\n",
       "      (conv7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): EmptyModule()\n",
       "    (12): Sequential(\n",
       "      (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (conv9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): EmptyModule()\n",
       "    (15): Sequential(\n",
       "      (conv10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (16): EmptyModule()\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): Sequential(\n",
       "      (conv11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky11): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (19): EmptyModule()\n",
       "    (20): Sequential(\n",
       "      (conv12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (conv13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (22): EmptyModule()\n",
       "    (23): Sequential(\n",
       "      (conv14): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): EmptyModule()\n",
       "    (25): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (26): Sequential(\n",
       "      (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky15): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Sequential(\n",
       "      (conv16): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (28): Sequential(\n",
       "      (conv17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (conv18): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (linear18): Identity()\n",
       "    )\n",
       "    (30): YoloLayer()\n",
       "    (31): EmptyModule()\n",
       "    (32): Sequential(\n",
       "      (conv19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): Upsample_expand()\n",
       "    (34): EmptyModule()\n",
       "    (35): Sequential(\n",
       "      (conv20): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Sequential(\n",
       "      (conv21): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (linear21): Identity()\n",
       "    )\n",
       "    (37): YoloLayer()\n",
       "  )\n",
       "  (loss): YoloLayer()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0foTUg1SO6eB",
    "outputId": "2cf7aa42-85a0-4404-b048-d4a76e731b3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77rUiNBAPBvp",
    "outputId": "f02f81c6-1c7d-4d2b-a68f-97aa27c94f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMBn44XvpePH"
   },
   "source": [
    "# Code for fusing - Our model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hsbl5s7_Lb_9"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6x615XbPRfY",
    "outputId": "ba4d98a9-6796-4343-bc9f-1218151efbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv1.bin\n",
      "Layer 1 feature map printed to ../bn1.bin\n",
      "Layer 2 feature map printed to ../leaky1.bin\n",
      "Fused weights of 1 printed to file ../Layer1/fused_conv1_bn1_weights.bin\n",
      "Fused biases of 1 printed to file ../Layer1/fused_conv1_bn1_bias.bin\n",
      "(32, 3, 3, 3)\n",
      "error: 0.000000000000659\n",
      "Image input stored in the file../Layer1/conv_layer1_input.bin\n",
      "Conv layer1 output stored in the file../Layer1/conv_layer1_output.bin\n"
     ]
    }
   ],
   "source": [
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = m.models[0]  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "inp = torch.ones(1, 3, 416, 416)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer1/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer1/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv = torch.nn.Conv2d(\n",
    "    model.conv1.in_channels,\n",
    "    model.conv1.out_channels,\n",
    "    kernel_size=model.conv1.kernel_size,\n",
    "    stride=model.conv1.stride,\n",
    "    padding=model.conv1.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer1/fused_conv1_bn1_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer1/fused_conv1_bn1_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "weight = buf1.reshape(fusedconv.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model = torch.nn.Sequential()\n",
    "new_model.append(fusedconv)\n",
    "new_model.append(model.leaky1)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"data/dog.jpg\")\n",
    "sized = cv2.resize(img, (m.width, m.height))\n",
    "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "y_original = model(img)\n",
    "y_original\n",
    "y_new = new_model(img)\n",
    "\n",
    "d = torch.mean(torch.pow(y_original - y_new, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "img_filename = \"../Layer1/conv_layer1_input.bin\"\n",
    "\n",
    "with open(img_filename, \"wb\") as f:\n",
    "    img.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + img_filename)\n",
    "\n",
    "out_filename = \"../Layer1/conv_layer1_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new.detach().numpy().tofile(f)\n",
    "print(\"Conv layer1 output stored in the file\" + out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nopk5-FRitb0",
    "outputId": "18d88bff-3d2a-46cd-b61e-44d8a20ae76a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003611326217651367\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "y__original = model(img)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(time_spent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltqsUdfup0g3"
   },
   "source": [
    "# Code to convert the bias and weights to torch params and copy to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmRiQ0yQp8WN"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goRjLysijcR7"
   },
   "source": [
    "# Layer 2 Convolution BN ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXwQaVfpMZrU"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGEJc0wbjiqU",
    "outputId": "30a1beb1-cf14-4931-951a-d6d44ff8d64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv2.bin\n",
      "Layer 1 feature map printed to ../bn2.bin\n",
      "Layer 2 feature map printed to ../leaky2.bin\n",
      "Fused weights of 2 printed to file ../Layer2/fused_conv2_bn2_weights.bin\n",
      "Fused biases of 2 printed to file ../Layer2/fused_conv2_bn2_bias.bin\n",
      "(64, 32, 3, 3)\n",
      "torch.Size([1, 32, 208, 208])\n",
      "error: 0.000000000000755\n",
      "Image input stored in the file../Layer2/conv_layer2_input.bin\n",
      "Conv layer2 output stored in the file../Layer2/conv_layer2_output.bin\n"
     ]
    }
   ],
   "source": [
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = m.models[1]  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "\n",
    "inp = torch.ones(1, 32, 208, 208)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer2/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer2/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv2 = torch.nn.Conv2d(\n",
    "    model.conv2.in_channels,\n",
    "    model.conv2.out_channels,\n",
    "    kernel_size=model.conv2.kernel_size,\n",
    "    stride=model.conv2.stride,\n",
    "    padding=model.conv2.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer2/fused_conv2_bn2_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer2/fused_conv2_bn2_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "\n",
    "weight = buf1.reshape(fusedconv2.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv2.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv2.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model = torch.nn.Sequential()\n",
    "new_model.append(fusedconv2)\n",
    "new_model.append(model.leaky2)\n",
    "\n",
    "inputfile = \"../Layer1/conv_layer1_output.bin\"\n",
    "\n",
    "fp1 = open(inputfile, \"rb\")\n",
    "buf2 = np.fromfile(fp1, dtype=np.float32)\n",
    "fp1.close()\n",
    "\n",
    "input = buf2.reshape((1, 32, 208, 208))\n",
    "input2 = torch.from_numpy(input)\n",
    "\n",
    "\n",
    "y_input_layer2 = m.models[0](img)\n",
    "\n",
    "print(y_input_layer2.shape)\n",
    "\n",
    "y_original = model(y_input_layer2)\n",
    "\n",
    "y_new = new_model(y_input_layer2)\n",
    "\n",
    "d = torch.mean(torch.pow(y_original - y_new, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "img_filename = \"../Layer2/conv_layer2_input.bin\"\n",
    "\n",
    "with open(img_filename, \"wb\") as f:\n",
    "    img.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + img_filename)\n",
    "\n",
    "out_filename = \"../Layer2/conv_layer2_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new.detach().numpy().tofile(f)\n",
    "print(\"Conv layer2 output stored in the file\" + out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ajeLXV7nLnw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn1yxvrUmLTR"
   },
   "outputs": [],
   "source": [
    "with open(\"../relu_output_2.bin\", \"rb\") as f:\n",
    "    y_new_other = np.reshape(np.fromfile(f, dtype=np.float32), (1, 64, 104, 104))\n",
    "\n",
    "y__new = torch.from_numpy(y_new_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeYvlyE0m-zw",
    "outputId": "bf8b5c14-f49d-4ffb-8cc0-7b50debdff6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.133518382906914\n"
     ]
    }
   ],
   "source": [
    "d = torch.mean(torch.pow(y__new - y_original, 2))\n",
    "print(\"error: %.15f\" % d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewRdrMmq1ftU"
   },
   "source": [
    "Layer - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es4Ie6CTN-At"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3JNKjmp2IL7",
    "outputId": "813cfbce-02e4-4281-f0df-9b158336eb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv3.bin\n",
      "Layer 1 feature map printed to ../bn3.bin\n",
      "Layer 2 feature map printed to ../leaky3.bin\n",
      "Fused weights of 3 printed to file ../Layer3/fused_conv3_bn3_weights.bin\n",
      "Fused biases of 3 printed to file ../Layer3/fused_conv3_bn3_bias.bin\n",
      "(64, 64, 3, 3)\n",
      "error: 0.000000000001471\n",
      "Image input stored in the file../Layer3/conv_layer3_input.bin\n",
      "Conv layer2 output stored in the file../Layer3/conv_layer3_output.bin\n"
     ]
    }
   ],
   "source": [
    "layer3_model = m.models[2]\n",
    "\n",
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = layer3_model  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "inp = torch.ones(1, 64, 104, 104)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer3/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer3/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv3 = torch.nn.Conv2d(\n",
    "    model.conv3.in_channels,\n",
    "    model.conv3.out_channels,\n",
    "    kernel_size=model.conv3.kernel_size,\n",
    "    stride=model.conv3.stride,\n",
    "    padding=model.conv3.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer3/fused_conv3_bn3_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer3/fused_conv3_bn3_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "weight = buf1.reshape(fusedconv3.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv3.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv3.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model3 = torch.nn.Sequential()\n",
    "new_model3.append(fusedconv3)\n",
    "new_model3.append(model.leaky3)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"data/dog.jpg\")\n",
    "sized = cv2.resize(img, (m.width, m.height))\n",
    "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "y1 = m.models[0](img)\n",
    "y2 = m.models[1](y1)\n",
    "y3 = m.models[2](y2)\n",
    "\n",
    "y_new3 = new_model3(y2)\n",
    "\n",
    "d = torch.mean(torch.pow(y3 - y_new3, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "layer3_input_filename = \"../Layer3/conv_layer3_input.bin\"\n",
    "\n",
    "with open(layer3_input_filename, \"wb\") as f:\n",
    "    y2.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer3_input_filename)\n",
    "\n",
    "out_filename = \"../Layer3/conv_layer3_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new3.detach().numpy().tofile(f)\n",
    "print(\"Conv layer2 output stored in the file\" + out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_kv5EKh3phj"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MX3pOxzH3r6d",
    "outputId": "83b73230-4505-4a1d-9e5e-fd12711a88a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmptyModule()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.models[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUItfFxN3ujp"
   },
   "outputs": [],
   "source": [
    "y4 = m.models[3](y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sja9ybmu3xmB",
    "outputId": "abd22c32-403a-4404-d372-350fb660b6d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 104, 104])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC5FP52D324e"
   },
   "outputs": [],
   "source": [
    "y5 = m.models[4](y4[:, 32:64, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeA83-dxDa4U"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at25ol1kENpS"
   },
   "source": [
    "# Conv Layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MODAeDVgDO0C",
    "outputId": "55f1acdb-161d-43cc-cdd7-2bb1610abff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv4.bin\n",
      "Layer 1 feature map printed to ../bn4.bin\n",
      "Layer 2 feature map printed to ../leaky4.bin\n",
      "Fused weights of 4 printed to file ../Layer4/fused_conv4_bn4_weights.bin\n",
      "Fused biases of 4 printed to file ../Layer4/fused_conv4_bn4_bias.bin\n",
      "(32, 32, 3, 3)\n",
      "error: 0.000000000001077\n",
      "error: 0.000000000001313\n",
      "Image input stored in the file../Layer4/conv_layer4_input.bin\n",
      "Image input stored in the file../Layer4/conv_layer4_input.bin\n",
      "Conv layer2 output stored in the file../Layer4/conv_layer4_output.bin\n",
      "Conv layer5 input stored in the file../Layer4/conv_layer5_input.bin\n"
     ]
    }
   ],
   "source": [
    "layer4_model = m.models[4]\n",
    "\n",
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = layer4_model  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "inp = torch.ones(1, 32, 104, 104)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer4/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer4/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv4 = torch.nn.Conv2d(\n",
    "    model.conv4.in_channels,\n",
    "    model.conv4.out_channels,\n",
    "    kernel_size=model.conv4.kernel_size,\n",
    "    stride=model.conv4.stride,\n",
    "    padding=model.conv4.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer4/fused_conv4_bn4_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer4/fused_conv4_bn4_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "weight = buf1.reshape(fusedconv4.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv4.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv4.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model4 = torch.nn.Sequential()\n",
    "new_model4.append(fusedconv4)\n",
    "new_model4.append(model.leaky4)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"data/dog.jpg\")\n",
    "sized = cv2.resize(img, (m.width, m.height))\n",
    "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "y1 = m.models[0](img)\n",
    "y2 = m.models[1](y1)\n",
    "y3 = m.models[2](y2)\n",
    "y4 = m.models[3](y3)\n",
    "y5 = m.models[4](y4[:, 32:64, :, :])\n",
    "\n",
    "y_new5 = new_model4(y4[:, 32:64, :, :])\n",
    "\n",
    "d = torch.mean(torch.pow(y5 - y_new5, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "\n",
    "y5_1 = m.models[4](y4[:, 0:32, :, :])\n",
    "y5_new_1 = new_model4(y4[:, 0:32, :, :])\n",
    "\n",
    "d1 = torch.mean(torch.pow(y5_1 - y5_new_1, 2))\n",
    "print(\"error: %.15f\" % d1)\n",
    "\n",
    "\n",
    "layer4_input_filename = \"../Layer4/conv_layer4_input.bin\"\n",
    "\n",
    "with open(layer4_input_filename, \"wb\") as f:\n",
    "    y4.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer4_input_filename)\n",
    "\n",
    "y4_ = y4[:, 32:64, :, :]\n",
    "\n",
    "layer4_input_2_filename = \"../Layer4/conv_layer4_inputs.bin\"\n",
    "with open(layer4_input_2_filename, \"wb\") as f:\n",
    "    y4_.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer4_input_filename)\n",
    "\n",
    "\n",
    "out_filename = \"../Layer4/conv_layer4_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new5.detach().numpy().tofile(f)\n",
    "print(\"Conv layer2 output stored in the file\" + out_filename)\n",
    "\n",
    "layer5_in_filename = \"../Layer4/conv_layer5_input.bin\"\n",
    "\n",
    "with open(layer5_in_filename, \"wb\") as f:\n",
    "    y_new5.detach().numpy().tofile(f)\n",
    "print(\"Conv layer5 input stored in the file\" + layer5_in_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv_Fkjt5E1gN"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Layer5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oodrm6QFO1c",
    "outputId": "27256904-a2b1-45b2-de65-3e707b8f282d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv5.bin\n",
      "Layer 1 feature map printed to ../bn5.bin\n",
      "Layer 2 feature map printed to ../leaky5.bin\n",
      "Fused weights of 5 printed to file ../Layer5/fused_conv5_bn5_weights.bin\n",
      "Fused biases of 5 printed to file ../Layer5/fused_conv5_bn5_bias.bin\n",
      "(32, 32, 3, 3)\n",
      "error: 0.000000000001383\n",
      "Image input stored in the file../Layer5/conv_layer5_input.bin\n",
      "Conv layer output stored in the file../Layer5/conv_layer5_output.bin\n",
      "Conv layer6 input stored in the file../Layer5/conv_layer6_input.bin\n"
     ]
    }
   ],
   "source": [
    "layer5_model = m.models[5]\n",
    "\n",
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = layer5_model  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "inp = torch.ones(1, 32, 104, 104)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer5/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer5/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv5 = torch.nn.Conv2d(\n",
    "    model.conv5.in_channels,\n",
    "    model.conv5.out_channels,\n",
    "    kernel_size=model.conv5.kernel_size,\n",
    "    stride=model.conv5.stride,\n",
    "    padding=model.conv5.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer5/fused_conv5_bn5_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer5/fused_conv5_bn5_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "weight = buf1.reshape(fusedconv5.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv5.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv5.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model5 = torch.nn.Sequential()\n",
    "new_model5.append(fusedconv5)\n",
    "new_model5.append(model.leaky5)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"data/dog.jpg\")\n",
    "sized = cv2.resize(img, (m.width, m.height))\n",
    "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "y1 = m.models[0](img)\n",
    "y2 = m.models[1](y1)\n",
    "y3 = m.models[2](y2)\n",
    "y4 = m.models[3](y3)\n",
    "y5 = m.models[4](y4[:, 32:64, :, :])\n",
    "y6 = m.models[5](y5)\n",
    "\n",
    "y_new6 = new_model5(y5)\n",
    "\n",
    "d = torch.mean(torch.pow(y6 - y_new6, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "layer5_input_filename = \"../Layer5/conv_layer5_input.bin\"\n",
    "\n",
    "with open(layer5_input_filename, \"wb\") as f:\n",
    "    y5.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer5_input_filename)\n",
    "\n",
    "out_filename = \"../Layer5/conv_layer5_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new6.detach().numpy().tofile(f)\n",
    "print(\"Conv layer output stored in the file\" + out_filename)\n",
    "\n",
    "layer5_in_filename = \"../Layer5/conv_layer6_input.bin\"\n",
    "\n",
    "with open(layer5_in_filename, \"wb\") as f:\n",
    "    y_new6.detach().numpy().tofile(f)\n",
    "print(\"Conv layer6 input stored in the file\" + layer5_in_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PHJpC81GZvj"
   },
   "outputs": [],
   "source": [
    "layer6_input = torch.cat((y6, y5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "Gll0csYoImnM",
    "outputId": "8aa3e540-04ad-4c85-b793-37513be2a4d7"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-65db80d461c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Layer6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../Layer6'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"../Layer6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0z6jL4EHUjw",
    "outputId": "7c9fc6c5-5bda-47b5-c265-7f87e4abde65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      ")\n",
      "Layer 0 feature map printed to ../conv6.bin\n",
      "Layer 1 feature map printed to ../bn6.bin\n",
      "Layer 2 feature map printed to ../leaky6.bin\n",
      "Fused weights of 6 printed to file ../Layer6/fused_conv6_bn6_weights.bin\n",
      "Fused biases of 6 printed to file ../Layer6/fused_conv6_bn6_bias.bin\n",
      "(64, 64, 1, 1)\n",
      "error: 0.000000000000155\n",
      "Image input stored in the file../Layer6/conv_layer6_input.bin\n",
      "Conv layer output stored in the file../Layer6/conv_layer6_output.bin\n",
      "Conv layer7 input stored in the file../Layer6/conv_layer7_input.bin\n"
     ]
    }
   ],
   "source": [
    "layer6_model = m.models[7]\n",
    "\n",
    "write_layer_outputs_to_file = True\n",
    "write_layer_params_to_file = True\n",
    "\n",
    "features = {}\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = layer6_model  # Change the model numbers here. This is the first layer now\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "inp = torch.ones(1, 64, 104, 104)\n",
    "\n",
    "raw_layers = []\n",
    "for layer in model.named_modules():\n",
    "    raw_layers.append(layer[0])\n",
    "\n",
    "leaf_layers = []\n",
    "for i in range(1, len(raw_layers) - 1):\n",
    "    curr_layer = raw_layers[i]\n",
    "    next_layer = raw_layers[i + 1]\n",
    "    if next_layer[: len(curr_layer) + 1] != curr_layer + \".\":\n",
    "        leaf_layers.append(curr_layer)\n",
    "leaf_layers.append(next_layer)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(leaf_layers)):\n",
    "    layers.append(re.sub(r\"\\.(\\d)\", r\"[\\1]\", leaf_layers[i]))\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    layer = layers[i]\n",
    "    layer_hook = (\n",
    "        \"model.\" + layer + \".register_forward_hook(get_features('\" + layer + \"'))\"\n",
    "    )\n",
    "    exec(layer_hook)\n",
    "\n",
    "# Run inference\n",
    "outp = model(inp)\n",
    "\n",
    "EPS = 10**-5  # constant\n",
    "\n",
    "if write_layer_outputs_to_file:\n",
    "    # Write layer outputs\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if layer in features.keys():\n",
    "            layer_name = layer.replace(\"].\", \"_\")\n",
    "            layer_name = layer_name.replace(\"[\", \"_\")\n",
    "            layer_name = layer_name.replace(\"]\", \"\")\n",
    "            filename = \"../\" + layer_name + \".bin\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                features[layer].cpu().numpy().tofile(f)\n",
    "            print(\"Layer \" + str(i) + \" feature map printed to \" + filename)\n",
    "\n",
    "if write_layer_params_to_file:\n",
    "    # Write layer params\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if \"conv\" in layer or \"downsample[0]\" in layer:\n",
    "            conv_layer_name = layer.replace(\"].\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"[\", \"_\")\n",
    "            conv_layer_name = conv_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_param_name = layer.replace(\"[\", \".\")\n",
    "            conv_param_name = conv_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            conv_weight = model.state_dict()[conv_param_name + \".weight\"]\n",
    "\n",
    "        if \"bn\" in layer or \"downsample[1]\" in layer:\n",
    "            bn_layer_name = layer.replace(\"].\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"[\", \"_\")\n",
    "            bn_layer_name = bn_layer_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_param_name = layer.replace(\"[\", \".\")\n",
    "            bn_param_name = bn_param_name.replace(\"]\", \"\")\n",
    "\n",
    "            bn_weight = model.state_dict()[bn_param_name + \".weight\"]\n",
    "            bn_bias = model.state_dict()[bn_param_name + \".bias\"]\n",
    "            bn_mean = model.state_dict()[bn_param_name + \".running_mean\"]\n",
    "            bn_var = model.state_dict()[bn_param_name + \".running_var\"]\n",
    "\n",
    "            bn_factor = torch.div(bn_weight, torch.sqrt(bn_var + EPS)).view(-1, 1, 1, 1)\n",
    "            fused_weight = torch.mul(conv_weight, bn_factor)\n",
    "            fused_bias = bn_bias - torch.div(\n",
    "                torch.mul(bn_weight, bn_mean), torch.sqrt(bn_var + EPS)\n",
    "            )\n",
    "\n",
    "            if \"downsample\" in bn_layer_name:\n",
    "                layer_number = \"0\"\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"downsample\")]\n",
    "            else:\n",
    "                layer_number = conv_layer_name[-1]\n",
    "                layer_prefix = bn_layer_name[0 : bn_layer_name.find(\"bn\")]\n",
    "\n",
    "            weights_filename = (\n",
    "                \"../Layer6/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_weights.bin\"\n",
    "            )  # This gives the bin for weights\n",
    "            bias_filename = (\n",
    "                \"../Layer6/fused_\"\n",
    "                + layer_prefix\n",
    "                + \"conv\"\n",
    "                + layer_number\n",
    "                + \"_bn\"\n",
    "                + layer_number\n",
    "                + \"_bias.bin\"\n",
    "            )  # This gives the bin for bias\n",
    "\n",
    "            with open(weights_filename, \"wb\") as f:\n",
    "                fused_weight.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused weights of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + weights_filename\n",
    "            )\n",
    "\n",
    "            with open(bias_filename, \"wb\") as f:\n",
    "                fused_bias.detach().numpy().tofile(f)\n",
    "            print(\n",
    "                \"Fused biases of \"\n",
    "                + layer_prefix\n",
    "                + layer_number\n",
    "                + \" printed to file \"\n",
    "                + bias_filename\n",
    "            )\n",
    "\n",
    "\n",
    "fusedconv6 = torch.nn.Conv2d(\n",
    "    model.conv6.in_channels,\n",
    "    model.conv6.out_channels,\n",
    "    kernel_size=model.conv6.kernel_size,\n",
    "    stride=model.conv6.stride,\n",
    "    padding=model.conv6.padding,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "weightfile1 = \"../Layer6/fused_conv6_bn6_bias.bin\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fp = open(weightfile1, \"rb\")\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "bias = buf\n",
    "\n",
    "weightfile2 = \"../Layer6/fused_conv6_bn6_weights.bin\"\n",
    "\n",
    "fp = open(weightfile2, \"rb\")\n",
    "buf1 = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "weight = buf1.reshape(fusedconv6.weight.data.shape)\n",
    "print(weight.shape)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "fusedconv6.weight.copy_(torch.from_numpy(weight))\n",
    "fusedconv6.bias.copy_(torch.from_numpy(bias))\n",
    "\n",
    "new_model6 = torch.nn.Sequential()\n",
    "new_model6.append(fusedconv6)\n",
    "new_model6.append(model.leaky6)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"data/dog.jpg\")\n",
    "sized = cv2.resize(img, (m.width, m.height))\n",
    "sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(sized.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "y1 = m.models[0](img)\n",
    "y2 = m.models[1](y1)\n",
    "y3 = m.models[2](y2)\n",
    "y4 = m.models[3](y3)\n",
    "y5 = m.models[4](y4[:, 32:64, :, :])\n",
    "y6 = m.models[5](y5)\n",
    "layer6_input = torch.cat((y6, y5), axis=1)\n",
    "y7 = m.models[7](layer6_input)\n",
    "\n",
    "y_new7 = new_model6(layer6_input)\n",
    "\n",
    "d = torch.mean(torch.pow(y7 - y_new7, 2))\n",
    "print(\"error: %.15f\" % d)\n",
    "\n",
    "layer6_input_filename = \"../Layer6/conv_layer6_input.bin\"\n",
    "\n",
    "with open(layer6_input_filename, \"wb\") as f:\n",
    "    layer6_input.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer6_input_filename)\n",
    "\n",
    "out_filename = \"../Layer6/conv_layer6_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y_new7.detach().numpy().tofile(f)\n",
    "print(\"Conv layer output stored in the file\" + out_filename)\n",
    "\n",
    "layer7_in_filename = \"../Layer6/conv_layer7_input.bin\"\n",
    "\n",
    "with open(layer7_in_filename, \"wb\") as f:\n",
    "    y_new7.detach().numpy().tofile(f)\n",
    "print(\"Conv layer7 input stored in the file\" + layer7_in_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq4hPPl2I-3R"
   },
   "outputs": [],
   "source": [
    "layer7_input = torch.cat((y_new7, y_new3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02Ov4uoaJbA-",
    "outputId": "285ead12-a2f4-407e-dbac-b12738635878"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 104, 104])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer7_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FjgVidvJe70",
    "outputId": "7042af3f-0384-4ca3-c6fc-8d8db9899cb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.models[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8DX_dX-KGIH"
   },
   "outputs": [],
   "source": [
    "y9 = m.models[9](layer7_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jS5POnDCKL1u",
    "outputId": "e290b668-7ce6-492d-c8fb-beef9ebfda9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.3152e-01, -4.9125e-02, -2.2711e-01,  ..., -2.9255e-02,\n",
       "           -3.1526e-02,  5.3563e+00],\n",
       "          [-1.5385e-02, -2.5106e-02, -2.6858e-01,  ..., -5.2941e-02,\n",
       "           -4.2126e-01, -1.2941e-01],\n",
       "          [ 1.1590e+00,  3.1158e-01, -9.0390e-02,  ...,  1.5182e+00,\n",
       "           -2.3955e-01, -3.0923e-01],\n",
       "          ...,\n",
       "          [-1.3615e-02,  2.0874e-01,  1.6401e-01,  ..., -7.4391e-02,\n",
       "           -3.6979e-02, -9.3041e-02],\n",
       "          [ 8.7522e-01,  8.9358e-01,  1.3700e+00,  ...,  1.6249e-01,\n",
       "           -8.2644e-02, -5.7963e-02],\n",
       "          [ 3.5337e+00,  2.6536e+00,  2.4756e+00,  ...,  2.0903e+00,\n",
       "            6.6415e-01, -6.5191e-02]],\n",
       "\n",
       "         [[-1.6508e-02,  1.9037e+00,  3.6852e-01,  ..., -3.2171e-01,\n",
       "           -9.5726e-03, -2.5075e-01],\n",
       "          [-1.0944e-01,  1.6691e+00,  4.1774e-01,  ..., -5.3844e-01,\n",
       "           -2.4751e-01, -4.8693e-02],\n",
       "          [-2.5944e-01, -6.3128e-02,  2.2200e-01,  ..., -2.7392e-01,\n",
       "           -6.2902e-01,  3.6915e-01],\n",
       "          ...,\n",
       "          [-3.3747e-01, -1.9336e-01, -2.6762e-01,  ..., -4.8672e-02,\n",
       "           -2.4837e-01, -2.0551e-01],\n",
       "          [-2.9246e-01, -2.1671e-01, -2.8737e-01,  ..., -1.0512e-01,\n",
       "           -4.9272e-01, -2.9753e-02],\n",
       "          [-3.9817e-02, -9.3271e-02, -2.9313e-01,  ..., -2.6863e-01,\n",
       "           -4.7877e-01,  8.3928e-01]],\n",
       "\n",
       "         [[-2.9558e-01, -3.0803e-01, -2.1966e-01,  ..., -5.2293e-03,\n",
       "           -5.6235e-01, -3.8504e-01],\n",
       "          [-1.7866e-01, -1.9440e-01,  1.4461e+00,  ..., -3.5492e-01,\n",
       "           -6.1765e-01, -4.0526e-01],\n",
       "          [-2.4638e-01, -2.3549e-01,  6.4000e-01,  ..., -3.3684e-01,\n",
       "           -7.7143e-01, -4.4605e-01],\n",
       "          ...,\n",
       "          [-5.0751e-01, -7.0380e-02, -6.2957e-02,  ..., -4.0311e-01,\n",
       "           -4.8887e-01, -2.6644e-01],\n",
       "          [-3.2516e-01,  8.9756e-02,  2.2525e+00,  ..., -3.0099e-01,\n",
       "           -4.1442e-01, -1.9113e-01],\n",
       "          [-7.4316e-02,  2.5611e-02, -1.3056e-02,  ..., -4.3077e-01,\n",
       "           -2.2805e-01, -3.0823e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7213e+00,  2.1309e+00,  1.8631e+00,  ...,  4.0740e+00,\n",
       "            3.9916e+00,  3.7150e+00],\n",
       "          [ 1.6043e+00,  1.7301e+00,  1.4657e+00,  ...,  2.9402e+00,\n",
       "            1.5737e+00,  5.1289e-01],\n",
       "          [ 1.4657e+00,  1.3684e+00,  1.0716e+00,  ...,  2.2412e+00,\n",
       "            4.0657e-01,  3.5209e-01],\n",
       "          ...,\n",
       "          [ 2.4917e+00,  2.7444e+00,  2.0445e+00,  ...,  1.5761e+00,\n",
       "            1.6297e+00,  1.8176e+00],\n",
       "          [ 2.5183e+00,  2.6301e+00,  2.0734e+00,  ...,  1.5244e+00,\n",
       "            1.7083e+00,  1.4888e+00],\n",
       "          [ 2.2007e+00,  2.4156e+00,  2.0040e+00,  ...,  1.7374e+00,\n",
       "            1.9316e+00,  1.4875e+00]],\n",
       "\n",
       "         [[ 2.1842e+00,  2.7000e+00,  2.5524e+00,  ..., -1.4079e-01,\n",
       "           -5.1378e-01, -1.8526e-02],\n",
       "          [ 2.7410e+00,  1.5492e+00,  1.4627e+00,  ..., -4.7352e-01,\n",
       "           -6.5204e-01, -4.4819e-01],\n",
       "          [ 2.8806e+00,  1.7301e+00,  1.4528e+00,  ..., -6.6494e-01,\n",
       "           -3.6188e-01, -3.1972e-01],\n",
       "          ...,\n",
       "          [ 1.5780e+00,  1.7501e-01,  2.8172e-01,  ...,  1.3799e+00,\n",
       "            1.4420e+00,  2.5987e+00],\n",
       "          [ 1.8171e+00,  8.9414e-01, -7.2231e-03,  ...,  1.4300e+00,\n",
       "            1.4791e+00,  1.9235e+00],\n",
       "          [ 1.0860e+00,  1.6141e+00,  3.8743e-01,  ...,  1.5666e+00,\n",
       "            1.5387e+00,  1.6125e+00]],\n",
       "\n",
       "         [[-2.9792e-01,  1.9811e+00,  2.2500e+00,  ..., -1.5981e-01,\n",
       "           -8.3205e-01, -7.6474e-01],\n",
       "          [-2.7153e-01,  1.5485e+00,  1.4590e+00,  ..., -2.5002e-01,\n",
       "           -9.3214e-02, -1.1375e+00],\n",
       "          [-3.0788e-01,  1.4442e+00,  1.2199e+00,  ..., -6.1012e-01,\n",
       "           -2.3291e-01, -4.6443e-01],\n",
       "          ...,\n",
       "          [-9.1181e-01,  3.3674e-01,  2.2895e-01,  ...,  1.1908e+00,\n",
       "            9.4710e-01,  1.3702e+00],\n",
       "          [-8.8076e-01,  1.3777e+00,  3.1798e-01,  ...,  5.2622e-01,\n",
       "            9.5084e-01,  5.1801e-01],\n",
       "          [-9.1187e-01,  2.1263e+00,  6.5637e-01,  ...,  1.7440e+00,\n",
       "            2.0287e+00, -8.2343e-02]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer7_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrJxIHEdKOXU",
    "outputId": "3350a1ac-937a-4513-9f3c-b021a9a4f059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.3152e-01, -2.2711e-01,  4.2286e-02,  ...,  4.4751e+00,\n",
       "            1.4639e+00,  5.3563e+00],\n",
       "          [ 1.5748e+00, -2.5769e-02, -3.9965e-01,  ..., -1.2106e-01,\n",
       "            1.5182e+00, -2.3955e-01],\n",
       "          [ 1.8654e+00,  3.0813e-01, -3.6903e-01,  ..., -2.3271e-02,\n",
       "           -1.8064e-01, -1.3758e-01],\n",
       "          ...,\n",
       "          [ 7.0710e-01,  9.6734e-01, -6.7745e-03,  ...,  5.4169e-01,\n",
       "            1.6281e-01, -7.6980e-02],\n",
       "          [ 5.0894e-01,  1.6401e-01, -9.3256e-02,  ...,  3.9575e-01,\n",
       "            6.0742e-01, -3.6979e-02],\n",
       "          [ 3.5337e+00,  2.4756e+00,  2.3717e+00,  ...,  2.0329e+00,\n",
       "            2.0903e+00,  6.6415e-01]],\n",
       "\n",
       "         [[ 1.9037e+00,  4.1774e-01,  3.8525e+00,  ...,  1.9832e+00,\n",
       "           -1.1867e-01, -9.5726e-03],\n",
       "          [-6.3128e-02,  2.2200e-01, -7.9004e-02,  ...,  5.8153e-01,\n",
       "            3.7357e-02,  6.8406e-01],\n",
       "          [-6.3146e-02, -2.4866e-02, -2.4733e-01,  ..., -2.6140e-02,\n",
       "           -2.1162e-01,  1.1614e+00],\n",
       "          ...,\n",
       "          [-1.7079e-01, -1.8706e-03,  5.5213e-01,  ..., -8.7509e-04,\n",
       "           -8.4524e-02,  1.2167e-01],\n",
       "          [-1.0531e-01, -2.3094e-01,  9.1622e-01,  ...,  2.7085e-01,\n",
       "           -4.8672e-02, -1.3601e-01],\n",
       "          [-3.9817e-02, -1.1924e-01,  3.1471e-01,  ..., -1.6780e-01,\n",
       "           -9.9137e-02,  8.3928e-01]],\n",
       "\n",
       "         [[-1.7866e-01,  1.4461e+00, -1.0030e-01,  ..., -5.0875e-02,\n",
       "           -5.2293e-03, -3.8504e-01],\n",
       "          [-2.3549e-01,  8.8404e-01, -3.1202e-01,  ...,  1.8661e+00,\n",
       "           -3.3684e-01, -4.4605e-01],\n",
       "          [-2.4382e-01,  5.4532e-01, -2.2299e-01,  ..., -3.4132e-01,\n",
       "           -1.1188e-01, -2.7770e-01],\n",
       "          ...,\n",
       "          [-1.6365e-01,  5.0366e-01, -1.1143e-01,  ..., -1.7580e-01,\n",
       "           -3.7306e-01, -3.9836e-01],\n",
       "          [-3.8052e-02,  5.3392e-02,  3.1245e-02,  ..., -1.2120e-01,\n",
       "           -3.6168e-01, -2.6644e-01],\n",
       "          [ 8.9756e-02,  2.2525e+00,  5.5293e-01,  ..., -7.7744e-02,\n",
       "           -2.9951e-01, -1.9113e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.1309e+00,  1.9545e+00,  2.8517e+00,  ...,  3.0593e+00,\n",
       "            4.0740e+00,  3.9916e+00],\n",
       "          [ 1.5725e+00,  1.2270e+00,  1.4728e+00,  ...,  2.2486e+00,\n",
       "            2.2976e+00,  7.9419e-01],\n",
       "          [ 1.6487e+00,  1.2130e+00,  1.4593e+00,  ...,  1.4395e+00,\n",
       "            8.0171e-01,  1.1721e+00],\n",
       "          ...,\n",
       "          [ 2.6263e+00,  2.5157e+00,  2.2774e+00,  ...,  2.0196e+00,\n",
       "            1.7593e+00,  1.9870e+00],\n",
       "          [ 2.9179e+00,  2.2377e+00,  2.0557e+00,  ...,  1.9061e+00,\n",
       "            1.7305e+00,  1.9660e+00],\n",
       "          [ 2.6301e+00,  2.0734e+00,  2.2356e+00,  ...,  1.8832e+00,\n",
       "            1.8150e+00,  1.9316e+00]],\n",
       "\n",
       "         [[ 2.7410e+00,  2.5524e+00,  3.3249e+00,  ..., -1.5151e-01,\n",
       "           -1.4079e-01, -1.8526e-02],\n",
       "          [ 2.8806e+00,  1.4842e+00,  1.9073e+00,  ..., -4.2135e-01,\n",
       "           -4.5617e-01, -3.1972e-01],\n",
       "          [ 2.9768e+00,  1.4679e+00,  3.0465e+00,  ..., -1.7148e-01,\n",
       "           -5.0012e-01,  5.1934e-01],\n",
       "          ...,\n",
       "          [ 2.0186e+00,  1.2843e+00,  1.7766e-01,  ...,  1.7071e+00,\n",
       "            1.5462e+00,  2.9865e+00],\n",
       "          [ 1.5780e+00,  6.0573e-01,  7.7551e-01,  ...,  1.7136e+00,\n",
       "            1.5446e+00,  2.8300e+00],\n",
       "          [ 1.8171e+00,  8.4549e-01,  9.5132e-01,  ...,  1.6352e+00,\n",
       "            1.5666e+00,  1.9235e+00]],\n",
       "\n",
       "         [[ 1.9811e+00,  2.2500e+00,  1.8660e+00,  ..., -6.7454e-01,\n",
       "           -1.5981e-01, -9.3214e-02],\n",
       "          [ 1.5138e+00,  1.4277e+00,  2.0971e+00,  ..., -5.3291e-01,\n",
       "           -2.5263e-01, -2.3291e-01],\n",
       "          [ 1.5483e+00,  1.2298e+00,  1.7860e+00,  ...,  1.7115e-01,\n",
       "           -2.8166e-01, -1.1855e-01],\n",
       "          ...,\n",
       "          [ 1.9490e-01,  5.3192e-01,  5.8427e-02,  ...,  1.3222e+00,\n",
       "            1.0866e+00,  1.4664e+00],\n",
       "          [ 3.3674e-01,  8.4887e-01,  9.0545e-02,  ...,  1.2446e+00,\n",
       "            1.2350e+00,  1.4077e+00],\n",
       "          [ 2.1263e+00,  7.2957e-01,  1.2936e+00,  ...,  2.2179e+00,\n",
       "            2.2438e+00,  2.0287e+00]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfFjNTuRUo0G"
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"../Maxpool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAYCFUXHUsQT",
    "outputId": "a5162a66-41ed-405a-857f-4c70d2b198eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input stored in the file../Maxpool/maxpool_input.bin\n",
      "Conv layer output stored in the file../Maxpool/maxpool_output.bin\n"
     ]
    }
   ],
   "source": [
    "layer7_input_filename = \"../Maxpool/maxpool_input.bin\"\n",
    "\n",
    "with open(layer7_input_filename, \"wb\") as f:\n",
    "    layer7_input.detach().numpy().tofile(f)\n",
    "print(\"Image input stored in the file\" + layer7_input_filename)\n",
    "\n",
    "out_filename = \"../Maxpool/maxpool_output.bin\"\n",
    "\n",
    "with open(out_filename, \"wb\") as f:\n",
    "    y9.detach().numpy().tofile(f)\n",
    "print(\"Conv layer output stored in the file\" + out_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}